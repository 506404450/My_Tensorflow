{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL.Image as PI\n",
    "from pylab import *\n",
    "DEFAULT_PADDING = \"SAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"/home/ghr/campaign/baidu big data/训练数据/data_label.txt\"\n",
    "val_path = \"/home/ghr/campaign/baidu big data/训练数据/val_label.txt\"\n",
    "model_path = \"/home/ghr/NoteBook/tensorflow-vgg-master/vgg16.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;用于读取预训练的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(data_path, session):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    for key in data_dict:\n",
    "        with tf.variable_scope(key, reuse=True):\n",
    "            for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                session.run(tf.get_variable(subkey).assign(data))\n",
    "\n",
    "def load_with_skip(data_path, session, skip_layer):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    for key in data_dict:\n",
    "        if key not in skip_layer:\n",
    "            with tf.variable_scope(key, reuse=True):\n",
    "                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                    session.run(tf.get_variable(subkey).assign(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;make_var用于创建变量,concat用于连接tensor,这里连接不同group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_var(name, shape):\n",
    "    return tf.get_variable(name, shape)\n",
    "    \n",
    "def concat(inputs, axis, name):\n",
    "    return tf.concat(values = inputs, axis = axis, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;k_h, k_w为kernel的尺寸;s_h, s_w为strides的尺寸;c_i为输入通道数,c_o为>卷积核数、输出通道数;name为层名;group > 1是将输入通道分为多组来卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv(input, k_h, k_w, c_o, s_h, s_w, name, relu = True, padding = DEFAULT_PADDING, group = 1):\n",
    "    c_i = input.get_shape()[-1].value\n",
    "    assert c_i % group == 0\n",
    "    assert c_o % group == 0        \n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding = padding)\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = make_var('weights', shape = [k_h, k_w, c_i / group, c_o])\n",
    "        biases = make_var('biases', [c_o])\n",
    "        if group == 1:\n",
    "            conv = convolve(input, kernel)\n",
    "        else:\n",
    "            input_groups = tf.split(input, group, 3) #分割input的第4维（通>道数）为group部分\n",
    "            kernel_groups = tf.split(kernel, group, 3)\n",
    "            output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "            conv = tf.concat(output_groups, 3)\n",
    "        conv_shape_list = conv.get_shape().as_list()\n",
    "        conv_shape_list[0] = -1\n",
    "        if relu:\n",
    "            bias = tf.reshape(tf.nn.bias_add(conv, biases), conv_shape_list)\n",
    "            return tf.nn.relu(bias, name = scope.name)\n",
    "    return tf.reshape(tf.nn.bias_add(conv, biases), conv_shape_list, name = scope.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;定义池化方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_pool(input, k_h, k_w, s_h, s_w, name, padding = DEFAULT_PADDING):\n",
    "    return tf.nn.max_pool(input,\n",
    "                          ksize = [1, k_h, k_w, 1],\n",
    "                          strides = [1, s_h, s_w, 1],\n",
    "                          padding = padding,\n",
    "                          name = name)\n",
    "\n",
    "def avg_pool(input, k_h, k_w, s_h, s_w, name, padding = DEFAULT_PADDING):\n",
    "    return tf.nn.avg_pool(input,\n",
    "                          ksize = [1, k_h, k_w, 1],\n",
    "                          strides = [1, s_h, s_w, 1],\n",
    "                          padding = padding,\n",
    "                          name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;定义fully connect层,softmax层和dropout层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(input, num_in, num_out, name, relu=True):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = make_var('weights', shape = [num_in, num_out])\n",
    "        biases = make_var('biases', [num_out])\n",
    "        op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n",
    "        fc = op(input, weights, biases, name = scope.name)\n",
    "        return fc\n",
    "\n",
    "def softmax(input, name):\n",
    "    return tf.nn.softmax(input, name = name)\n",
    "\n",
    "def dropout(input, keep_prob):\n",
    "    return tf.nn.dropout(input, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;定义vggnet16网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = tf.placeholder(dtype = tf.float32, shape = [None, 224, 224, 3])\n",
    "ys = tf.placeholder(dtype = tf.float32, shape = [None, 133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv1_1 = conv(xs, 3, 3, 64, 1, 1, name = \"conv1_1\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv1_2 = conv(conv1_1, 3, 3, 64, 1, 1, name = \"conv1_2\", relu = True, padding = \"SAME\", group = 1)\n",
    "max_pool1_1 = max_pool(conv1_2, 2, 2, 2, 2, name = \"max_pool1_1\" , padding = \"SAME\")\n",
    "\n",
    "conv2_1 = conv(max_pool1_1, 3, 3, 128, 1, 1, name = \"conv2_1\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv2_2 = conv(conv2_1, 3, 3, 128, 1, 1, name = \"conv2_2\", relu = True, padding = \"SAME\", group = 1)\n",
    "max_pool2_1 = max_pool(conv2_1, 2, 2, 2, 2, name = \"max_pool2_1\" , padding = \"SAME\")\n",
    "\n",
    "conv3_1 = conv(max_pool2_1, 3, 3, 256, 1, 1, name = \"conv3_1\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv3_2 = conv(conv3_1, 3, 3, 256, 1, 1, name = \"conv3_2\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv3_3 = conv(conv3_2, 3, 3, 256, 1, 1, name = \"conv3_3\", relu = True, padding = \"SAME\", group = 1)\n",
    "max_pool3_1 = max_pool(conv3_3, 2, 2, 2, 2, name = \"max_pool3_1\", padding = \"SAME\")\n",
    "\n",
    "conv4_1 = conv(max_pool3_1, 3, 3, 512, 1, 1, name = \"conv4_1\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv4_2 = conv(conv4_1, 3, 3, 512, 1, 1, name = \"conv4_2\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv4_3 = conv(conv4_2, 3, 3, 512, 1, 1, name = \"conv4_3\", relu = True, padding = \"SAME\", group = 1)\n",
    "max_pool4_1 = max_pool(conv4_3, 2, 2, 2, 2, name = \"max_pool4_1\", padding = \"SAME\")\n",
    "\n",
    "conv5_1 = conv(max_pool4_1, 3, 3, 512, 1, 1, name = \"conv5_1\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv5_2 = conv(conv5_1, 3, 3, 512, 1, 1, name = \"conv5_2\", relu = True, padding = \"SAME\", group = 1)\n",
    "conv5_3 = conv(conv5_2, 3, 3, 512, 1, 1, name = \"conv5_3\", relu = True, padding = \"SAME\", group = 1)\n",
    "max_pool5_1 = max_pool(conv5_3, 2, 2, 2, 2, name = \"max_pool5_1\", padding = \"SAME\")\n",
    "\n",
    "wh = max_pool5_1.get_shape().as_list()\n",
    "fc_in_num = wh[1] * wh[2] * wh[3]\n",
    "max_pool5_1 = tf.reshape(max_pool5_1, [-1, fc_in_num])\n",
    "fc6 = fc(max_pool5_1, fc_in_num, 4096, name = \"fc6\", relu = True)\n",
    "dropout6 = dropout(fc6, 0.5)\n",
    "\n",
    "fc7 = fc(dropout6, 4096, 4096, name = \"fc7\", relu = True)\n",
    "dropout7 = dropout(fc7, 0.5)\n",
    "\n",
    "fc8 = fc(dropout7, 4096, 133, name = \"fc8\", relu = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = softmax(fc8, name = \"softmax\")\n",
    "cross_entropy = -tf.reduce_mean(ys * tf.log(pred))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(ys,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = open(train_path)\n",
    "lines = train_file.readlines()\n",
    "train_index = []\n",
    "train_label_index = []\n",
    "train_file.close()\n",
    "for l in lines:\n",
    "    items = l.split()\n",
    "    train_index.append(items[0] + \" \" + items[1] + \" \" + items[2])\n",
    "    train_label_index.append(items[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_ptr = 0\n",
    "train_label_ptr = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iter in range(2000):\n",
    "        batch_train_index = train_index[train_ptr:train_ptr + batch_size]\n",
    "        batch_train_label_index = train_label_index[train_ptr:train_ptr + batch_size]\n",
    "        train_ptr = train_ptr + batch_size\n",
    "        images = np.ndarray([batch_size, 224, 224, 3]) \n",
    "        one_hot_labels = np.zeros((batch_size, 133))\n",
    "        for step in range(batch_size):\n",
    "            img = array(PI.open(batch_train_index[step]).resize([224, 224]))\n",
    "            images[step] = img\n",
    "            one_hot_labels[step][int(batch_train_label_index[step])] = 1\n",
    "        sess.run(train, feed_dict = {xs:images, ys:one_hot_labels})        \n",
    "        print(sess.run(accuracy, feed_dict = {xs:images, ys:one_hot_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16 = np.load(\"/home/ghr/NoteBook/tensorflow-vgg-master/vgg16.npy\", encoding='latin1').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
